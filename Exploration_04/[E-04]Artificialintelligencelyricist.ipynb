{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22ec23d7",
   "metadata": {},
   "source": [
    "# 작사가 인공지능 만들기\n",
    "\n",
    "▶목차\n",
    "\n",
    "\n",
    "|   Chapter               | details             |\n",
    "|:-------------------------|:---------------------|\n",
    "|4-7 |프로젝트: 작사가 인공지능 만들기         |\n",
    "|Step 1 | 데이터 다운로드 |\n",
    "|Step 2 | 데이터 읽어오기       |\n",
    "|Step 3 | 데이터 정제               |\n",
    "|Step 4 |평가 데이터셋 분리                  |\n",
    "|Step 5 |인공지능 만들기                  |\n",
    "|회고 | 느낀점 & 다양한 시도     |\n",
    "|Final |깃허브 주소 업로드                           |\n",
    "\n",
    "▶딥러닝 순서\n",
    "\n",
    "\n",
    "  | 딥러닝순서                                             | 상세내용                                       |\n",
    "|:------------------------------------------------------|:------------------------------------------------|\n",
    "|1. 데이터 준비       |데이터셋을 구성한다                |\n",
    "|2. 딥러닝 네트워크 설계   |데이터셋의 다양성, 정규화 등의 시도가 적절하였음|\n",
    "|3. 학습|학습모델을 사용하여 학습시킨다.                             |\n",
    "|4. 테스트(평가)| 학습한 내용을 파악하고 예측을 시도해본다|\n",
    "\n",
    "▶루브릭 평가 기준\n",
    "\n",
    "    \n",
    "- 아래의 기준을 바탕으로 프로젝트를 평가합니다.\n",
    "\n",
    "| 평가문항                                             | 상세기준                                       |\n",
    "|:------------------------------------------------------|:------------------------------------------------|\n",
    "|1. 가사 텍스트 생성 모델이 정상적으로 동작하는가?   |텍스트 제너레이션 결과가 그럴듯한 문장으로 생성되는가?                |\n",
    "|2. 데이터의 전처리와 데이터셋 구성 과정이 체계적으로 진행되었는가?   |특수문자 제거, 토크나이저 생성, 패딩처리 등의 과정이 빠짐없이 진행되었는가?|\n",
    "|3. 텍스트 생성모델이 안정적으로 학습되었는가?|텍스트 생성모델의 validation loss가 2.2 이하로 낮아졌는가?|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff62cfa7",
   "metadata": {},
   "source": [
    "## 1. 데이터 다운로드\n",
    "\n",
    "### 폴더 생성 후 데이터 이동\n",
    "\n",
    "$ mkdir -p ~/aiffel/lyricist/models\n",
    "$ ln -s ~/data ~/aiffel/lyricist/data\n",
    "\n",
    "## 2. 데이터 읽어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77401b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " [\"Now I've heard there was a secret chord\", 'That David played, and it pleased the Lord', \"But you don't really care for music, do you?\", 'It goes like this', 'The fourth, the fifth', 'The minor fall, the major lift', 'The baffled king composing Hallelujah Hallelujah', 'Hallelujah', 'Hallelujah', 'Hallelujah Your faith was strong but you needed proof']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os, re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus에 담습니다.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:10]) # 앞에서부터 10라인만 화면에 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8beeba6",
   "metadata": {},
   "source": [
    "# 3. 데이터 정제\n",
    "\n",
    "## 3-1. 정규표현식을 이용한 corpus 생성\n",
    "- tf.keras.preprocessing.text.Tokenizer를 이용해 corpus를 텐서로 변환\n",
    "- tf.data.Dataset.from_tensor_slices()를 이용해 corpus 텐서를 tf.data.Dataset객체로 변환\n",
    "- 정제함수 = Corpus 사용법\n",
    "- Corpus = [ ], ex) Corpus [:10], Corpus.append() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "430dae09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> the purification function is is corpus . <end>\n",
      "<start> now i ve heard there was a secret chord <end>\n",
      "<start> that david played , and it pleased the lord <end>\n",
      "<start> but you don t really care for music , do you ? <end>\n",
      "<start> it goes like this <end>\n",
      "<start> the fourth , the fifth <end>\n",
      "<start> the minor fall , the major lift <end>\n",
      "<start> the baffled king composing hallelujah hallelujah <end>\n",
      "<start> hallelujah <end>\n"
     ]
    }
   ],
   "source": [
    "# 입력된 문장을\n",
    "#     1. 소문자로 바꾸고, 양쪽 공백을 지웁니다\n",
    "#     2. 특수문자 양쪽에 공백을 넣고\n",
    "#     3. 여러개의 공백은 하나의 공백으로 바꿉니다\n",
    "#     4. a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꿉니다\n",
    "#     5. 다시 양쪽 공백을 지웁니다\n",
    "#     6. 문장 시작에는 <start>, 끝에는 <end>를 추가합니다\n",
    "# 이 순서로 처리해주면 문제가 되는 상황을 방지할 수 있겠네요!\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() # 1\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 2\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # 4\n",
    "    sentence = sentence.strip() # 5\n",
    "    sentence = '<start> ' + sentence + ' <end>' # 6\n",
    "    return sentence\n",
    "\n",
    "# 이 문장이 어떻게 필터링되는지 확인해 보세요.\n",
    "print(preprocess_sentence(\"The purification function @_is ;;; is corpus.\"))\n",
    "print(preprocess_sentence(raw_corpus[0]))\n",
    "print(preprocess_sentence(raw_corpus[1]))\n",
    "print(preprocess_sentence(raw_corpus[2]))\n",
    "print(preprocess_sentence(raw_corpus[3]))\n",
    "print(preprocess_sentence(raw_corpus[4]))\n",
    "print(preprocess_sentence(raw_corpus[5]))\n",
    "print(preprocess_sentence(raw_corpus[6]))\n",
    "print(preprocess_sentence(raw_corpus[7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52d7e659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> you saw her bathing on the roof <end>',\n",
       " '<start> her beauty and the moonlight overthrew her <end>',\n",
       " '<start> she tied you <end>',\n",
       " '<start> to a kitchen chair <end>',\n",
       " '<start> she broke your throne , and she cut your hair <end>',\n",
       " '<start> and from your lips she drew the hallelujah hallelujah <end>',\n",
       " '<start> hallelujah <end>',\n",
       " '<start> hallelujah <end>',\n",
       " '<start> hallelujah you say i took the name in vain <end>',\n",
       " '<start> i don t even know the name <end>']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 여기에 정제된 문장을 모을겁니다\n",
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    # 우리가 원하지 않는 문장은 건너뜁니다\n",
    "    if len(sentence) == 0 or len(sentence.split()) > 15: continue   # 길이가 0이거나 단어가 15개 넘는 긴 문장은 제외\n",
    "    if sentence[-1] == \":\": continue\n",
    "    \n",
    "    # 정제를 하고 담아주세요\n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    corpus.append(preprocessed_sentence)\n",
    "        \n",
    "# 정제된 결과를 10개만 확인해보죠\n",
    "corpus[10:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f779ac",
   "metadata": {},
   "source": [
    "## Tokenizer 패키지로 corpus를 텐서로 변환\n",
    "### 단어장의 크기는 12,000 이상 으로 설정하세요 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ef40a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   50    5 ...    0    0    0]\n",
      " [   2   17 2706 ...    0    0    0]\n",
      " [   2   34    7 ...   44    3    0]\n",
      " ...\n",
      " [   2  259  194 ...   12    3    0]\n",
      " [   5   22    9 ...   10 1099    3]\n",
      " [   2    7   33 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7ff5a17fea90>\n"
     ]
    }
   ],
   "source": [
    "# 토큰화 할 때 텐서플로우의 Tokenizer와 pad_sequences를 사용합니다\n",
    "# 더 잘 알기 위해 아래 문서들을 참고하면 좋습니다\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences\n",
    "def tokenize(corpus):\n",
    "    # 12,000단어를 기억할 수 있는 tokenizer를 만들겁니다\n",
    "    # 우리는 이미 문장을 정제했으니 filters가 필요없어요\n",
    "    # 12,000단어에 포함되지 못한 단어는 '<unk>'로 바꿀거에요\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=12000, \n",
    "        filters=' ',\n",
    "        oov_token=\"<unk>\"\n",
    "    )\n",
    "    # corpus를 이용해 tokenizer 내부의 단어장을 완성합니다\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    # 준비한 tokenizer를 이용해 corpus를 Tensor로 변환합니다\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞춰줍니다\n",
    "    # 만약 시퀀스가 짧다면 문장 뒤에 패딩을 붙여 길이를 맞춰줍니다.\n",
    "    # 문장 앞에 패딩을 붙여 길이를 맞추고 싶다면 padding='pre'를 사용합니다\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post', maxlen=15)  \n",
    "  \n",
    "    \n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f221ecd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   50    5   91  307   62   57    9  957 5739    3    0    0    0\n",
      "     0]\n",
      " [   2   17 2706  879    4    8   11 6171    6  347    3    0    0    0\n",
      "     0]\n",
      " [   2   34    7   35   15  162  283   28  334    4   48    7   44    3\n",
      "     0]\n",
      " [   2   11  346   24   42    3    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [   2    6 4332    4    6 2172    3    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [   2    6 5740  292    4    6 1211  790    3    0    0    0    0    0\n",
      "     0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(168357, 15)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 변환된 텐서 확인\n",
    "print(tensor[:6, :20] )\n",
    "tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42109a28",
   "metadata": {},
   "source": [
    "### tokenizer에 구축된 단어 사전의 인덱스입니다. 단어 사전이 어떻게 구축되었는지 아래와 같이 확인해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb49f6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : i\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n"
     ]
    }
   ],
   "source": [
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b39526",
   "metadata": {},
   "source": [
    "2번 인덱스가 바로 <start>였습니다.\n",
    "왜 모든 행이 2로 시작하는지 이해할 수 있겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "86b4abf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2   50    5   91  307   62   57    9  957 5739    3    0    0    0]\n",
      "[  50    5   91  307   62   57    9  957 5739    3    0    0    0    0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(168357, 14)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# source(소스문장), target(정답문장) 생성\n",
    "src_input = tensor[:, :-1]   # tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성합니다. 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높습니다.\n",
    "tgt_input = tensor[:, 1:]    # tensor에서 <start>를 잘라내서 타겟 문장을 생성합니다.\n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])\n",
    "src_input.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d97db4c",
   "metadata": {},
   "source": [
    "# 4. 평가 데이터셋 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "553f359f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (134685, 14)\n",
      "Target Train: (134685, 14)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# tensor를 train, test 데이터로 분리\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size=0.2)\n",
    "\n",
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db50f91c",
   "metadata": {},
   "source": [
    "# 5. 모델 설계 및 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "013f8fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 256)         3072256   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, None, 1024)        5246976   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, None, 1024)        8392704   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, None, 12001)       12301025  \n",
      "=================================================================\n",
      "Total params: 29,012,961\n",
      "Trainable params: 29,012,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = tokenizer.num_words + 1  # 단어사전의 단어 개수 + 0:<pad>\n",
    "embedding_size = 256\n",
    "hidden_size = 1024\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, embedding_size))\n",
    "model.add(tf.keras.layers.LSTM(hidden_size, return_sequences=True))\n",
    "model.add(tf.keras.layers.LSTM(hidden_size, return_sequences=True))\n",
    "model.add(tf.keras.layers.Dense(vocab_size))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e3df4767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4209/4209 [==============================] - 139s 31ms/step - loss: 3.2191\n",
      "Epoch 2/10\n",
      "4209/4209 [==============================] - 134s 32ms/step - loss: 2.7693\n",
      "Epoch 3/10\n",
      "4209/4209 [==============================] - 134s 32ms/step - loss: 2.4927\n",
      "Epoch 4/10\n",
      "4209/4209 [==============================] - 134s 32ms/step - loss: 2.2462\n",
      "Epoch 5/10\n",
      "4209/4209 [==============================] - 134s 32ms/step - loss: 2.0301\n",
      "Epoch 6/10\n",
      "4209/4209 [==============================] - 134s 32ms/step - loss: 1.8426\n",
      "Epoch 7/10\n",
      "4209/4209 [==============================] - 134s 32ms/step - loss: 1.6799\n",
      "Epoch 8/10\n",
      "4209/4209 [==============================] - 133s 32ms/step - loss: 1.5399\n",
      "Epoch 9/10\n",
      "4209/4209 [==============================] - 133s 32ms/step - loss: 1.4206\n",
      "Epoch 10/10\n",
      "4209/4209 [==============================] - 133s 32ms/step - loss: 1.3194\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff5940272e0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# optimization 최적화, loss 손실, Epochs 반복학습 횟수 지정\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(enc_train, dec_train, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9ff70b",
   "metadata": {},
   "source": [
    "# 5. 모델 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "680e4047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 생성 함수\n",
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 일단 텐서로 변환합니다.\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 텍스트를 실제로 생성할때는 루프를 돌면서 단어 하나씩 생성해야 합니다. \n",
    "    while True:\n",
    "        predict = model(test_tensor)  # 입력받은 문장의 텐서를 입력합니다. \n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]   # 우리 모델이 예측한 마지막 단어가 바로 새롭게 생성한 단어가 됩니다. \n",
    "\n",
    "        # 우리 모델이 새롭게 예측한 단어를 입력 문장의 뒤에 붙여 줍니다. \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "\n",
    "        # 우리 모델이 <end>를 예측했거나, max_len에 도달하지 않았다면  while 루프를 또 돌면서 다음 단어를 예측해야 합니다.\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # 생성된 tensor 안에 있는 word index를 tokenizer.index_word 사전을 통해 실제 단어로 하나씩 변환합니다. \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated   # 이것이 최종적으로 모델이 생성한 자연어 문장입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9a8e03",
   "metadata": {},
   "source": [
    "# 언어모델 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a1f83332",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you , liberian girl <end> '"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "657f895e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i need a girl from <unk> <end> '"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i need\", max_len=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a9806d04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i wanna be with you , i wanna be with you <end> '"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i wanna\", max_len=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "52d1fa38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> the like that i could claim <end> '"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> the like\", max_len=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5f3ba40d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> you like sleeping alone <end> '"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> you like\", max_len=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a80d321b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> and you re the one <end> '"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> and you\", max_len=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e7227d16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> to you i m not <end> '"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> to you\", max_len=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2786a273",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i lucky that you re the only one <end> '"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i lucky\", max_len=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278ff6df",
   "metadata": {},
   "source": [
    "# 회고\n",
    "> 이게 RNN(순환신경망)인가 싶은데요\n",
    "아직도 갈 길은 멀고 이해를 바로 하기에는 많은 부분이 어렵네요.\n",
    "뭔가 할 듯 하면서도 모르는 것 같고 모르는 것 같으면서도 알 것 같은 이 묘한 상황\n",
    "어디서 부터 어디까지 배워야 하는 건지 감을 못 잡고 있어서 배워야 할 것은 참 많다라는 생각이 듭니다.\n",
    "반복학습 에포크는 너무 오래 걸림을 다시한번 더 느꼈습니다.\n",
    "\n",
    "# 마무리\n",
    "- 단어장의 크기는 12,000 으로 설정\n",
    "- 길이가 0이거나 단어가 15개 넘는 긴 문장은 제외\n",
    "- Epoch 10/10 \n",
    "- loss: 1.3194\n",
    "loss 값은 1.31로 낮게 나왔다.\n",
    "\n",
    "\n",
    "#### reference\n",
    "https://github.com/kec0130/AIFFEL-project/blob/main/exploration/E6_lyricist.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
